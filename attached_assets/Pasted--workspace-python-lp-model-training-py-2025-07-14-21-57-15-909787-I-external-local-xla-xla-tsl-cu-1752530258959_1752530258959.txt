~/workspace$ python lp_model_training.py
2025-07-14 21:57:15.909787: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-07-14 21:57:15.913154: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-07-14 21:57:15.923337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1752530235.937310    8194 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1752530235.941162    8194 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1752530235.952542    8194 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1752530235.952558    8194 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1752530235.952560    8194 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1752530235.952565    8194 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2025-07-14 21:57:15.956062: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-07-14 21:57:18,220 | INFO | Initializing data module...
2025-07-14 21:57:18.236708: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
2025-07-14 21:57:18,261 | INFO | Starting training...
2025-07-14 21:57:18,397 | INFO | Training fold 1/5 (train=1243, val=311)
Epoch 1/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 3s 115ms/step - auc: 0.4983 - loss: 0.7034 - val_auc: 0.5449 - val_loss: 0.7019 - learning_rate: 0.0010
Epoch 2/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 15ms/step - auc: 0.6722 - loss: 0.6952 - val_auc: 0.6066 - val_loss: 0.6987 - learning_rate: 0.0010
Epoch 3/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 14ms/step - auc: 0.7857 - loss: 0.6877 - val_auc: 0.6866 - val_loss: 0.6936 - learning_rate: 0.0010
Epoch 4/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - auc: 0.9003 - loss: 0.6730 - val_auc: 0.7464 - val_loss: 0.6839 - learning_rate: 0.0010
Epoch 5/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - auc: 0.9476 - loss: 0.6458 - val_auc: 0.7739 - val_loss: 0.6651 - learning_rate: 0.0010
Epoch 6/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 0.9799 - loss: 0.5865 - val_auc: 0.7834 - val_loss: 0.6334 - learning_rate: 0.0010
Epoch 7/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 0.9921 - loss: 0.4779 - val_auc: 0.7907 - val_loss: 0.5901 - learning_rate: 0.0010
Epoch 8/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 0.9988 - loss: 0.3003 - val_auc: 0.7950 - val_loss: 0.5524 - learning_rate: 0.0010
Epoch 9/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - auc: 0.9999 - loss: 0.1260 - val_auc: 0.7974 - val_loss: 0.5516 - learning_rate: 0.0010
Epoch 10/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - auc: 1.0000 - loss: 0.0357 - val_auc: 0.8022 - val_loss: 0.5974 - learning_rate: 0.0010
Epoch 11/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - auc: 1.0000 - loss: 0.0161 - val_auc: 0.8005 - val_loss: 0.6296 - learning_rate: 0.0010
Epoch 12/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step - auc: 1.0000 - loss: 0.0122 - val_auc: 0.8007 - val_loss: 0.6942 - learning_rate: 0.0010
Epoch 13/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0124 - val_auc: 0.8027 - val_loss: 0.7965 - learning_rate: 0.0010
Epoch 14/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0111 - val_auc: 0.7984 - val_loss: 0.7311 - learning_rate: 0.0010
Epoch 15/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - auc: 1.0000 - loss: 0.0114 - val_auc: 0.7948 - val_loss: 0.7265 - learning_rate: 0.0010
Epoch 16/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 11ms/step - auc: 1.0000 - loss: 0.0106 - val_auc: 0.7944 - val_loss: 0.7434 - learning_rate: 0.0010
Epoch 17/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0108 - val_auc: 0.7935 - val_loss: 0.7565 - learning_rate: 0.0010
Epoch 18/100
1/5 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - auc: 1.0000 - loss: 0.0121
Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 12ms/step - auc: 1.0000 - loss: 0.0112 - val_auc: 0.7883 - val_loss: 0.7606 - learning_rate: 0.0010
Epoch 19/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0104 - val_auc: 0.7880 - val_loss: 0.7657 - learning_rate: 5.0000e-04
Epoch 20/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0106 - val_auc: 0.7876 - val_loss: 0.7713 - learning_rate: 5.0000e-04
Epoch 21/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 9ms/step - auc: 1.0000 - loss: 0.0104 - val_auc: 0.7876 - val_loss: 0.7793 - learning_rate: 5.0000e-04
Epoch 22/100
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0103 - val_auc: 0.7870 - val_loss: 0.7844 - learning_rate: 5.0000e-04
Epoch 23/100
1/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0105
Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.
5/5 ━━━━━━━━━━━━━━━━━━━━ 0s 10ms/step - auc: 1.0000 - loss: 0.0104 - val_auc: 0.7872 - val_loss: 0.7857 - learning_rate: 5.0000e-04
Epoch 23: early stopping
Restoring model weights from the end of the best epoch: 13.
2025-07-14 21:57:23.478682: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence
2025-07-14 21:57:23,481 | INFO | Fold 1 AUC: 0.8028
2025-07-14 21:57:23,481 | ERROR | Error in fold 1: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf
2025-07-14 21:57:23,481 | ERROR | Application error: Training failed in fold 1: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf
Traceback (most recent call last):
  File "/home/runner/workspace/lp_model_training.py", line 421, in train_cv
    model.save(model_path, save_format='tf', include_optimizer=False)
  File "/home/runner/workspace/.pythonlibs/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py", line 122, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File "/home/runner/workspace/.pythonlibs/lib/python3.11/site-packages/keras/src/saving/saving_api.py", line 69, in save_model
    raise ValueError(
ValueError: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/runner/workspace/lp_model_training.py", line 548, in <module>
    main()
  File "/home/runner/workspace/lp_model_training.py", line 519, in main
    metrics = trainer.train_cv(k=5)
              ^^^^^^^^^^^^^^^^^^^^^
  File "/home/runner/workspace/lp_model_training.py", line 425, in train_cv
    raise ModelError(f"Training failed in fold {fold}: {str(e)}")
ModelError: Training failed in fold 1: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf
~/workspace$ 